{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook: breast cancer diagnosis using multiple machine learning techniques:\n",
    "1. XGBoost (dominates hackathons and challenges such as Kaggle competitions)\n",
    "2. Random forest classifier\n",
    "2. Support vector machine\n",
    "4. k-nearest neighbors\n",
    "4. Deep learning\n",
    "5. Principal component analysis (visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Matrix operations\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "\n",
    "# Support vector machine model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas.plotting import scatter_matrix\n",
    "from xgboost import plot_tree, plot_importance, to_graphviz\n",
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydot \n",
    "\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SMOTE sampling\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# PCA for visualization purposes\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# deep learning\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input, BatchNormalization, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from IPython.display import Image\n",
    "from keras.utils.np_utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sklearn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"breast-cancer-wisconsin-original.csv\"\n",
    "attributes = ['id','diagnosis','radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean',\n",
    "              'compactness_mean','concavity_mean','concave_points_mean','symmetry_mean','fractal_dimension_mean',\n",
    "              'radius_se','texture_se','perimeter_se','area_se','smoothness_se','compactness_se','concavity_se',\n",
    "              'concave points_se','symmetry_se','fractal_dimension_se','radius_worst','texture_worst',\n",
    "              'perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst','concave_points_worst',\n",
    "              'symmetry_worst','fractal_dimension_worst']\n",
    "\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer = filepath,\n",
    "    names = attributes)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is relatively a small amount of data so we will see how well the models perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\n",
    "    labels = ['id'],\n",
    "    axis = 1,\n",
    "    inplace = True)\n",
    "\n",
    "df['diagnosis'] = pd.get_dummies(df['diagnosis'])\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('diagnosis').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['diagnosis'])\n",
    "df = np.array(df.drop(['diagnosis'], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversample with SMOTE (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = np.array(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size = 0.25)\n",
    "\n",
    "sm = SMOTE(\n",
    "    sampling_strategy = 'not majority',\n",
    "    k_neighbors = 5,\n",
    "    n_jobs = 1,\n",
    "    random_state = 12,\n",
    "    ratio = 1.0)\n",
    "\n",
    "X_res, y_res = sm.fit_sample(\n",
    "    X_train,\n",
    "    y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train length: ' + str(len(X_train)))\n",
    "print('X_test length: ' + str(len(X_test)))\n",
    "print('X_res length: ' + str(len(X_res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "XGBoost, just like a random forest classifier, is tree based and does not require feature scaling. Additionally, XGBoost uses boosting trees as opposed to gradient descent as another reason scaling is unncessary. \n",
    "\n",
    "- https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf\n",
    "- https://github.com/dmlc/xgboost/issues/357\n",
    "- https://pypi.org/project/xgboost/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use grid search to find the best parameters (won't experiment with many and put early stopping at 10 rounds for the sake of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import sklearn\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators':[10, 20, 50, 100, 500, 1000],\n",
    "    'max_depth': [3,4,5,10,20],\n",
    "    'learning_rate': [1e-1, 1e-2, 1e-3, 1e-4]}\n",
    "\n",
    "model = GridSearchCV(\n",
    "    model,\n",
    "    parameters,\n",
    "    cv = 10,\n",
    "    return_train_score = True,\n",
    "    iid = False)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)],\n",
    "    early_stopping_rounds = 10,\n",
    "    eval_metric=[\"error\", \"logloss\"],\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters according to grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_xgb = model.best_estimator_\n",
    "\n",
    "optimized_xgb.fit(\n",
    "    X_train,\n",
    "    y_train, \n",
    "    eval_metric=[\"error\", \"logloss\"])\n",
    "\n",
    "prediction_xgb = optimized_xgb.predict(X_test)\n",
    "\n",
    "print('Test accuracy: ' + str(accuracy_score(y_test,prediction_xgb)))\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    prediction_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = optimized_xgb.evals_result()\n",
    "\n",
    "epochs = len(results['validation_0']['error'])\n",
    "\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(\n",
    "    x_axis,\n",
    "    results['validation_0']['logloss'],\n",
    "    label='Train')\n",
    "\n",
    "ax.plot(\n",
    "    x_axis,\n",
    "    results['validation_1']['logloss'],\n",
    "    label='Test')\n",
    "\n",
    "ax.legend()\n",
    "plt.ylabel('Log Loss')\n",
    "plt.title('XGBoost Log Loss')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    x_axis,\n",
    "    results['validation_0']['error'],\n",
    "    label='Train')\n",
    "\n",
    "ax.plot(\n",
    "    x_axis,\n",
    "    results['validation_1']['error'],\n",
    "    label='Test')\n",
    "\n",
    "ax.legend()\n",
    "plt.ylabel('Error')\n",
    "plt.title('XGBoost Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import pydot\n",
    "mpl.rcParams['figure.dpi'] = 500\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import plot_tree, plot_importance, to_graphviz\n",
    "\n",
    "predictions = optimized_xgb.predict(X_test)\n",
    "\n",
    "print('Test accuracy: ' + str(accuracy_score(y_test,predictions)))\n",
    "print('')\n",
    "print('')\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    predictions))\n",
    "\n",
    "plot_importance(optimized_xgb)\n",
    "\n",
    "plot_tree(optimized_xgb, num_trees=2, rankdir='LR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "https://stackoverflow.com/questions/8961586/do-i-need-to-normalize-or-scale-data-for-randomforest-r-package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators = 500,\n",
    "    random_state = 42) \n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train)\n",
    "\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy: ' + str(accuracy_score(y_test,predictions)))\n",
    "print('')\n",
    "print('')\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(predictions)\n",
    "correct = 0\n",
    "for pred, actual in zip(predictions,y_test):\n",
    "    if pred == actual:\n",
    "        correct += 1\n",
    "        \n",
    "print('Prediction accuracy: ' + str(correct/float(total)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO  \n",
    "import pydot \n",
    "\n",
    "dot_data = StringIO() \n",
    "tree.export_graphviz(model[1], out_file=dot_data) \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "\n",
    "graph[0].write_pdf(\"tree.pdf\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (using grid search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machine, being distance based, requires the data to be scaled so no feature takes precedence over another. Scale the training data between 0,1 and scale the test data with the training data scaling parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train before scaling: \")\n",
    "print(X_train)\n",
    "print('')\n",
    "print(\"X_test before scaling: \")\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "\n",
    "scaler = preprocessing.MinMaxScaler() \n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "print(scaler.data_max_)\n",
    "print(scaler.data_min_)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train after scaling: \")\n",
    "print(X_train)\n",
    "print('')\n",
    "print(\"X_test after scaling: \")\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use grid search to find the best kernel and value of C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma = 'auto')\n",
    "\n",
    "parameters = {\n",
    "    'kernel':('linear', 'rbf'),\n",
    "    'C':[0.1, 0.5, 1, 2, 5, 10, 100, 1000]}\n",
    "\n",
    "clf = GridSearchCV(\n",
    "    clf, parameters,\n",
    "    cv = 10,\n",
    "    return_train_score = True,\n",
    "    iid = False,\n",
    "    verbose = True)\n",
    "\n",
    "clf.fit(\n",
    "    X_train,\n",
    "    y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_svm = clf.best_estimator_\n",
    "\n",
    "optimized_svm.fit(\n",
    "    X_train,\n",
    "    y_train)\n",
    "\n",
    "cv_results = model_selection.cross_val_score(\n",
    "    optimized_svm,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv = 10,\n",
    "    scoring='accuracy')\n",
    "\n",
    "prediction_svm = optimized_svm.predict(X_test)\n",
    "\n",
    "print('Test accuracy: ' + str(accuracy_score(y_test,prediction_svm)))\n",
    "print('')\n",
    "print('')\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    prediction_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To visualize the hyperplanes, use PCA to reduce to 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "\n",
    "principalComponents = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_svm.fit(\n",
    "    principalComponents,\n",
    "    y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision boundary from grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(principalComponents[:, 0],\n",
    "            principalComponents[:, 1],\n",
    "            s = 30,\n",
    "            cmap = plt.cm.Paired)\n",
    "\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "xx = np.linspace(\n",
    "    start = xlim[0],\n",
    "    stop = xlim[1],\n",
    "    num = 30)\n",
    "\n",
    "yy = np.linspace(\n",
    "    start = ylim[0],\n",
    "    stop = ylim[1],\n",
    "    num = 30)\n",
    "\n",
    "YY, XX = np.meshgrid(\n",
    "    yy,\n",
    "    xx)\n",
    "\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['-', '-', '-'])\n",
    "\n",
    "ax.scatter(optimized_svm.support_vectors_[:, 0], optimized_svm.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "plt.title('Linear Decision Boundary')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of curiousity, the radial basis function boundary is plotted below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clf = SVC(\n",
    "    gamma = 'auto',\n",
    "    C = 1.0,\n",
    "    kernel = 'rbf')\n",
    "\n",
    "clf.fit(principalComponents,\n",
    "        y_train)\n",
    "\n",
    "h = 0.2\n",
    "x_min, x_max = principalComponents[:,0].min() - 1, principalComponents[:, 0].max() + 1\n",
    "y_min, y_max = principalComponents[:,1].min() - 1, principalComponents[:, 1].max() + 1\n",
    "\n",
    "xx, yy = np.meshgrid(\n",
    "    np.arange(x_min, x_max, h),\n",
    "    np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = clf.predict(\n",
    "    np.c_[xx.ravel(),\n",
    "          yy.ravel()])\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(\n",
    "    xx,\n",
    "    yy,\n",
    "    Z,\n",
    "    cmap = plt.cm.coolwarm,\n",
    "    alpha = 0.8)\n",
    "\n",
    "plt.scatter(\n",
    "    principalComponents[:,0],\n",
    "    principalComponents[:,1],\n",
    "    c = y_train)\n",
    "\n",
    "plt.title('Radial Basis Function Kernel Boundaries')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors = 15)\n",
    "\n",
    "neigh.fit(\n",
    "    X_train,\n",
    "    y_train) \n",
    "\n",
    "results = neigh.predict(X_test)\n",
    "\n",
    "print('Test accuracy: ' + str(accuracy_score(y_test,results)))\n",
    "print('')\n",
    "print('')\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot adapted from sklearn: https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_classification.html#sphx-glr-auto-examples-neighbors-plot-nca-classification-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "h = .02\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "\n",
    "    clf = neighbors.KNeighborsClassifier(\n",
    "        n_neighbors = 15,\n",
    "        weights = weights)\n",
    "    \n",
    "    clf.fit(\n",
    "        principalComponents,\n",
    "        y_train)\n",
    "\n",
    "    x_min, x_max = principalComponents[:, 0].min() - 1, principalComponents[:, 0].max() + 1\n",
    "    y_min, y_max = principalComponents[:, 1].min() - 1, principalComponents[:, 1].max() + 1\n",
    "    \n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(x_min, x_max, h),\n",
    "        np.arange(y_min, y_max, h))\n",
    "  \n",
    "    Z = clf.predict(\n",
    "        np.c_[xx.ravel(),\n",
    "        yy.ravel()])\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(\n",
    "        xx,\n",
    "        yy,\n",
    "        Z,\n",
    "        cmap = cmap_light)\n",
    "\n",
    "    plt.scatter(\n",
    "        principalComponents[:, 0],\n",
    "        principalComponents[:, 1],\n",
    "        c = y_train,\n",
    "        cmap=cmap_bold,\n",
    "        edgecolor='k', s = 20)\n",
    "    \n",
    "    plt.xlim(\n",
    "        xx.min(),\n",
    "        xx.max())\n",
    "    \n",
    "    plt.ylim(\n",
    "        yy.min(),\n",
    "        yy.max())\n",
    "    \n",
    "    plt.title(\"2 Class Classification (k = 15)\")\n",
    "              \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 15\n",
    "\n",
    "# split for train, test\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state = seed,\n",
    "    shuffle = True)\n",
    "\n",
    "Y_train = to_categorical(y_train,\n",
    "                         num_classes = None)\n",
    "\n",
    "Y_test = to_categorical(y_test,\n",
    "                        num_classes = None)\n",
    "\n",
    "# split for train, validation\n",
    "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    test_size=0.3,\n",
    "    random_state = seed,\n",
    "    shuffle = True)\n",
    "\n",
    "# Upsampling\n",
    "sm = SMOTE(\n",
    "    sampling_strategy='not majority',\n",
    "    k_neighbors = 10,\n",
    "    n_jobs = 1,\n",
    "    random_state = 12,\n",
    "    ratio = 0.7)\n",
    "\n",
    "X_res, Y_res = sm.fit_sample(X_train, Y_train)\n",
    "\n",
    "Y_res = to_categorical(Y_res,\n",
    "                        num_classes = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(30,))\n",
    "\n",
    "x = Dense(\n",
    "    units = 96,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    use_bias = True,\n",
    "    bias_initializer='zeros')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('softmax')(x)\n",
    "\n",
    "skip1 = x\n",
    "\n",
    "x = Dense(\n",
    "    units = 96,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    use_bias = True,\n",
    "    bias_initializer='zeros')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.Add()([x, skip1])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "skip2 = x \n",
    "\n",
    "x = Dropout(rate = 0.3)(x)\n",
    "\n",
    "x = Dense(\n",
    "    units = 96,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    use_bias = True,\n",
    "    bias_initializer='zeros')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.Add()([x, skip2])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dropout(rate = 0.3)(x)\n",
    "\n",
    "x = Dense(\n",
    "    units = 96,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    use_bias = True,\n",
    "    bias_initializer='zeros')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.Add()([x, skip1])\n",
    "x = keras.layers.Add()([x, skip2])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dropout(rate = 0.3)(x)\n",
    "\n",
    "y = Dense(\n",
    "    units = 2,\n",
    "    activation='softmax')(x)\n",
    "\n",
    "model = Model(\n",
    "    inputs = input_layer,\n",
    "    outputs = y)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performed on oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr= 2e-4)\n",
    "\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    optimizer = opt,\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor = 'val_loss',\n",
    "    factor = 0.5,\n",
    "    patience = 5, \n",
    "    min_lr = 5e-7,\n",
    "    verbose = 1)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 10,\n",
    "    verbose = 1,\n",
    "    restore_best_weights = True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_res,\n",
    "    Y_res,                   \n",
    "    epochs = 300, \n",
    "    batch_size = 4,\n",
    "    verbose = 1,\n",
    "    validation_data = (X_val, Y_val),\n",
    "    callbacks = [reduce_lr,\n",
    "                early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print('Test accuracy: ' + str(accuracy_score(np.argmax(Y_test, axis=1),np.argmax(predictions, axis=1))))\n",
    "print('')\n",
    "print('')\n",
    "print(classification_report(np.argmax(Y_test, axis=1),np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performed on non-oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(30,))\n",
    "\n",
    "x = Dense(\n",
    "    units = 96,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    use_bias = True,\n",
    "    bias_initializer='zeros')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('softmax')(x)\n",
    "\n",
    "skip1 = x\n",
    "\n",
    "x = Dense(\n",
    "    units = 96,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    use_bias = True,\n",
    "    bias_initializer='zeros')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.Add()([x, skip1])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "skip2 = x \n",
    "\n",
    "x = Dropout(rate = 0.3)(x)\n",
    "\n",
    "x = Dense(\n",
    "    units = 96,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    use_bias = True,\n",
    "    bias_initializer='zeros')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.Add()([x, skip2])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dropout(rate = 0.3)(x)\n",
    "\n",
    "x = Dense(\n",
    "    units = 96,\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    use_bias = True,\n",
    "    bias_initializer='zeros')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = keras.layers.Add()([x, skip1])\n",
    "x = keras.layers.Add()([x, skip2])\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "x = Dropout(rate = 0.3)(x)\n",
    "\n",
    "y = Dense(\n",
    "    units = 2,\n",
    "    activation='softmax')(x)\n",
    "\n",
    "model = Model(\n",
    "    inputs = input_layer,\n",
    "    outputs = y)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = SGD(lr= 2e-4)\n",
    "\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    optimizer = opt,\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor = 'val_loss',\n",
    "    factor = 0.5,\n",
    "    patience = 5, \n",
    "    min_lr = 5e-7,\n",
    "    verbose = 1)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 10,\n",
    "    verbose = 1,\n",
    "    restore_best_weights = True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_res,\n",
    "    Y_res,                   \n",
    "    epochs = 300, \n",
    "    batch_size = 4,\n",
    "    verbose = 1,\n",
    "    validation_data = (X_val, Y_val),\n",
    "    callbacks = [reduce_lr,\n",
    "                early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print('Test accuracy: ' + str(accuracy_score(np.argmax(Y_test, axis=1),np.argmax(predictions, axis=1))))\n",
    "print('')\n",
    "print('')\n",
    "print(classification_report(np.argmax(Y_test, axis=1),np.argmax(predictions, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
